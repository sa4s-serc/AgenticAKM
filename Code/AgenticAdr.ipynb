{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cdc3cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\LAB\\ADR\\AgenticAdr\n"
     ]
    }
   ],
   "source": [
    "cd D:\\LAB\\ADR\\AgenticAdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502afb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Union\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109e0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCaller:\n",
    "    \"\"\"\n",
    "    A model-agnostic class for interacting with various LLMs (Gemini, GPT-5, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-pro\"):\n",
    "        print(f\"Initializing LLMCaller with model: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "\n",
    "        if model_name.startswith(\"gpt\"):\n",
    "            # Initialize OpenAI client only if needed\n",
    "            self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def call(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Sends a prompt to the selected LLM and returns the text response.\n",
    "        \"\"\"\n",
    "        if self.model_name == \"gemini-2.5-pro\":\n",
    "            llm = genai.GenerativeModel(self.model_name)\n",
    "            response = llm.generate_content(prompt)\n",
    "            return response.text\n",
    "\n",
    "        elif self.model_name == \"gpt-5\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-5\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            # print(\"ran with gpt-5\")\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38a7cf",
   "metadata": {},
   "source": [
    "### RepoSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f39ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepoSummarizer:\n",
    "    \"\"\"\n",
    "    An agent to summarize a code repository using an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-pro\"):\n",
    "        \"\"\"\n",
    "        Initializes the summarizer with a specific Gemini model.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing with model: {model_name}\")\n",
    "        self.model = LLMCaller(model_name)\n",
    "        self.ignore_patterns = [\n",
    "            '.git', '__pycache__', 'node_modules', 'dist', 'build',\n",
    "            '.DS_Store', '*.pyc', '*.log', '*.tmp', '*.swp'\n",
    "        ]\n",
    "        self.text_file_extensions = [\n",
    "            '.py', '.js', '.ts', '.java', '.c', '.cpp', '.h', '.hpp', '.cs',\n",
    "            '.go', '.rs', '.rb', '.php', '.html', '.css', '.scss', '.json',\n",
    "            '.xml', '.yaml', '.yml', '.md', '.txt', '.sh', '.toml', '.ini',\n",
    "            'Dockerfile', 'Makefile'\n",
    "        ]\n",
    "\n",
    "\n",
    "    def _is_text_file(self, file_path):\n",
    "        \"\"\"Checks if a file is likely a text file based on its extension.\"\"\"\n",
    "        return any(file_path.name.endswith(ext) for ext in self.text_file_extensions)\n",
    "\n",
    "\n",
    "    def _get_repo_structure(self, repo_path):\n",
    "        \"\"\"\n",
    "        Creates a string representation of the repository's file structure.\n",
    "        \"\"\"\n",
    "        tree_str = \"\"\n",
    "        for root, dirs, files in os.walk(repo_path):\n",
    "            # Filter out ignored directories\n",
    "            dirs[:] = [d for d in dirs if d not in self.ignore_patterns]\n",
    "            \n",
    "            level = root.replace(repo_path, '').count(os.sep)\n",
    "            indent = ' ' * 4 * level\n",
    "            tree_str += f\"{indent}ðŸ“ {os.path.basename(root)}/\\n\"\n",
    "            \n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                if f not in self.ignore_patterns:\n",
    "                    tree_str += f\"{sub_indent}ðŸ“„ {f}\\n\"\n",
    "        return tree_str\n",
    "\n",
    "\n",
    "    def _get_dependencies(self, repo_path):\n",
    "        \"\"\"\n",
    "        Finds and reads common dependency files.\n",
    "        \"\"\"\n",
    "        dependency_files = {\n",
    "            \"Python\": \"requirements.txt\",\n",
    "            \"Node.js\": \"package.json\",\n",
    "            \"Java (Maven)\": \"pom.xml\",\n",
    "            \"Java (Gradle)\": \"build.gradle\",\n",
    "            \"Ruby\": \"Gemfile\",\n",
    "        }\n",
    "        found_deps = \"No common dependency files found.\"\n",
    "        for tech, filename in dependency_files.items():\n",
    "            path = Path(repo_path) / filename\n",
    "            if path.exists():\n",
    "                try:\n",
    "                    found_deps = f\"--- {tech} Dependencies ({filename}) ---\\n\"\n",
    "                    found_deps += path.read_text(encoding='utf-8') + \"\\n\\n\"\n",
    "                except Exception as e:\n",
    "                    found_deps += f\"Could not read {filename}: {e}\\n\\n\"\n",
    "        return found_deps\n",
    "\n",
    "\n",
    "    def _summarize_key_files(self, repo_path, num_files=5):\n",
    "        \"\"\"\n",
    "        Finds the largest text files and generates a summary for each.\n",
    "        \"\"\"\n",
    "        file_sizes = []\n",
    "        for root, _, files in os.walk(repo_path):\n",
    "            if any(part in root for part in self.ignore_patterns):\n",
    "                continue\n",
    "            for file in files:\n",
    "                file_path = Path(root) / file\n",
    "                if self._is_text_file(file_path):\n",
    "                    try:\n",
    "                        size = file_path.stat().st_size\n",
    "                        if size > 100: # Ignore very small files\n",
    "                            file_sizes.append((file_path, size))\n",
    "                    except FileNotFoundError:\n",
    "                        continue\n",
    "\n",
    "        # Sort files by size in descending order and get the top N\n",
    "        file_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "        key_files = [f[0] for f in file_sizes[:num_files]]\n",
    "\n",
    "        print(f\"\\nSummarizing the {len(key_files)} largest files...\")\n",
    "        summaries = []\n",
    "        for file_path in key_files:\n",
    "            relative_path = file_path.relative_to(repo_path)\n",
    "            print(f\"  - Reading: {relative_path}\")\n",
    "            try:\n",
    "                content = file_path.read_text(encoding='utf-8')\n",
    "                if len(content.strip()) == 0:\n",
    "                    continue\n",
    "\n",
    "                prompt = f\"\"\"\n",
    "                Analyze the following code from the file '{relative_path}'.\n",
    "                Provide a concise, high-level summary (2-3 sentences) of its purpose and key functionality.\n",
    "\n",
    "                ```\n",
    "                {content[:4000]}\n",
    "                ```\n",
    "                \"\"\"\n",
    "                response = self.model.call(prompt)\n",
    "                summaries.append(f\"ðŸ“„ **File: {relative_path}**\\n{response}\\n\")\n",
    "            except Exception as e:\n",
    "                summaries.append(f\"ðŸ“„ **File: {relative_path}**\\n   - Could not summarize: {e}\\n\")\n",
    "\n",
    "        return \"\\n\".join(summaries)\n",
    "\n",
    "\n",
    "    def summarize_repo(self, repo_path: str, feedback: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes a repository from a local path, using optional feedback to improve accuracy.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(repo_path):\n",
    "            return \"Error: Provided repository path is not a directory.\"\n",
    "\n",
    "        # Add a header for regeneration attempts\n",
    "        if feedback:\n",
    "            print(\"\\nRe-generating summary with feedback...\")\n",
    "        else:\n",
    "            print(\"\\nAnalyzing repository structure...\")\n",
    "        \n",
    "        structure = self._get_repo_structure(repo_path)\n",
    "        dependencies = self._get_dependencies(repo_path)\n",
    "        file_summaries = self._summarize_key_files(repo_path)\n",
    "\n",
    "        feedback_prompt_section = \"\"\n",
    "        if feedback:\n",
    "            feedback_prompt_section = f\"\"\"\n",
    "            --- PREVIOUS ATTEMPT FEEDBACK ---\n",
    "            A previous attempt to summarize this repository was found to be inaccurate.\n",
    "            Use the following feedback to create a new, more accurate summary.\n",
    "            Do not repeat the previous mistakes.\n",
    "\n",
    "            Feedback: {feedback}\n",
    "            ---\n",
    "            \"\"\"\n",
    "\n",
    "        final_prompt = f\"\"\"\n",
    "        You are a senior software architect...\n",
    "        Based on the information provided below, generate a comprehensive, high-level summary.\n",
    "        {feedback_prompt_section}\n",
    "        --- REPOSITORY INFORMATION ---\n",
    "        **1. File & Directory Structure:**\n",
    "        ```\n",
    "        {structure[:4000]}\n",
    "        ```\n",
    "        **2. Detected Dependencies:**\n",
    "        ```\n",
    "        {dependencies}\n",
    "        ```\n",
    "        **3. Summaries of Key Files:**\n",
    "        ```\n",
    "        {file_summaries}\n",
    "        ```\n",
    "        --- END OF INFORMATION ---\n",
    "        Provide the final summary in a clear, well-structured markdown format.\n",
    "        \"\"\"\n",
    "        final_response = self.model.call(final_prompt)\n",
    "        return final_response\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee711f8",
   "metadata": {},
   "source": [
    "### SummaryChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9067a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryCheckerAgent:\n",
    "    \"\"\"\n",
    "    Verifies a summary and provides actionable feedback for correction if it's inaccurate.\n",
    "    \"\"\"\n",
    "    def __init__(self,  model_name: str = \"gemini-2.5-pro\"):\n",
    "        print(\"Initializing SummaryCheckerAgent...\")\n",
    "        self.model = LLMCaller(model_name)\n",
    "\n",
    "    def verify_summary(self, summary: str, repo_path: str) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Verifies the summary and generates corrective feedback if needed.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - A boolean: True if the summary is correct, False otherwise.\n",
    "            - A string: A justification if correct, or actionable feedback if incorrect.\n",
    "        \"\"\"\n",
    "        print(f\"Verifying summary against the code in {repo_path}...\")\n",
    "        tree_str = \"\"\n",
    "        # (The logic to get tree_str remains the same as before)\n",
    "        for root, dirs, files in os.walk(repo_path):\n",
    "            dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules']]\n",
    "            level = root.replace(repo_path, '').count(os.sep)\n",
    "            indent = ' ' * 4 * level\n",
    "            tree_str += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                tree_str += f\"{sub_indent}{f}\\n\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a meticulous code reviewer. Your task is to determine if the given **Summary**\n",
    "        accurately reflects the provided **Repository Context**.\n",
    "\n",
    "        **Repository Context (File and Directory Structure):**\n",
    "        ---\n",
    "        {tree_str[:4000]}\n",
    "        ---\n",
    "\n",
    "        **Summary to Verify:**\n",
    "        ---\n",
    "        {summary}\n",
    "        ---\n",
    "\n",
    "        First, provide your verdict as a single word: **CORRECT** or **INCORRECT**.\n",
    "        - If the verdict is **CORRECT**, follow it with a colon and a brief justification.\n",
    "        - If the verdict is **INCORRECT**, follow it with a colon and then, on a new line, provide actionable **FEEDBACK** for the summarizer agent on how to fix the summary. This feedback should be a clear instruction.\n",
    "\n",
    "        Example CORRECT response:\n",
    "        CORRECT: The summary accurately identifies this as a Java project.\n",
    "\n",
    "        Example INCORRECT response:\n",
    "        INCORRECT: The summary incorrectly identifies this as a Python project.\n",
    "        FEEDBACK: The repository contains `pom.xml` and Java source files, not Python files. Please regenerate the summary identifying the project as a Java Maven application.\n",
    "        \"\"\"\n",
    "        response = self.model.call(prompt)\n",
    "        result_text = response.strip()\n",
    "        \n",
    "        is_correct = result_text.startswith(\"CORRECT\")\n",
    "        feedback = result_text\n",
    "\n",
    "        if is_correct:\n",
    "            print(f\"-> Verification Result: CORRECT\")\n",
    "        else:\n",
    "            print(f\"-> Verification Result: INCORRECT\")\n",
    "        \n",
    "        return is_correct, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff43a3",
   "metadata": {},
   "source": [
    "### AdrWriterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93ced37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Model for an ADR ---\n",
    "class ADR(BaseModel):\n",
    "    \"\"\"Represents a single Architecture Decision Record.\"\"\"\n",
    "    title: str\n",
    "    context: str\n",
    "    decision: str\n",
    "    # Accept either a simple string or a structured dictionary for consequences\n",
    "    consequences: Union[str, Dict[str, List[str]]]\n",
    "\n",
    "# --- ADR Writer Agent ---\n",
    "class AdrWriterAgent:\n",
    "    \"\"\"\n",
    "    An agent that analyzes a repository summary to identify, format,\n",
    "    and save key design decisions as Architecture Decision Records (ADRs).\n",
    "    \"\"\"\n",
    "    def __init__(self,  model_name: str = \"gemini-2.5-pro\"):\n",
    "        print(\"Initializing AdrWriterAgent...\")\n",
    "        self.model = LLMCaller(model_name)\n",
    "\n",
    "\n",
    "    def _extract_design_decisions(self, summary: str, feedback: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Uses the LLM to extract design decisions, with a more robust prompt.\n",
    "        \"\"\"\n",
    "        if feedback:\n",
    "            print(\"Re-extracting design decisions with new feedback...\")\n",
    "        else:\n",
    "            print(\"Extracting design decisions from the summary...\")\n",
    "\n",
    "        feedback_prompt_section = \"\"\n",
    "        if feedback:\n",
    "            feedback_prompt_section = f\"\"\"\n",
    "            --- PREVIOUS ATTEMPT FEEDBACK ---\n",
    "            A previous attempt to generate ADRs was found to be inaccurate.\n",
    "            Use the following feedback to create a new, more accurate set of ADRs based on the summary.\n",
    "\n",
    "            Feedback: {feedback}\n",
    "            ---\n",
    "            \"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert senior software architect. Your task is to analyze the provided repository summary\n",
    "        and extract the most critical architectural and technological design decisions.\n",
    "        {feedback_prompt_section}\n",
    "        **Repository Summary**:\n",
    "        ---\n",
    "        {summary}\n",
    "        ---\n",
    "\n",
    "        Return your response as a valid JSON array of objects.\n",
    "        **It is critical that each object in the array has the following four string keys exactly as written: `title`, `context`, `decision`, `consequences`.**\n",
    "        \n",
    "        Do not use any other key names. For example, do not use `decision_point` instead of `title`. The format must be strictly followed.\n",
    "        \"\"\"\n",
    "        response = self.model.call(prompt)\n",
    "        clean_response = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        try:\n",
    "            decisions = json.loads(clean_response)\n",
    "            print(f\"Successfully extracted {len(decisions)} design decisions.\")\n",
    "            return decisions\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: The model did not return a valid JSON. Raw response:\\n{clean_response}\")\n",
    "            # Return an empty list to prevent crashes, allowing the feedback loop to potentially correct it.\n",
    "            return []\n",
    "\n",
    "    def _format_adrs(self, adrs: List[ADR]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Formats a list of ADR objects into a list of markdown strings.\n",
    "        This method NO LONGER saves files.\n",
    "        \"\"\"\n",
    "        if not adrs:\n",
    "            return []\n",
    "\n",
    "        formatted_adrs_list = []\n",
    "        for i, adr in enumerate(adrs, start=1):\n",
    "            consequences_text = \"\"\n",
    "            if isinstance(adr.consequences, dict):\n",
    "                pros = adr.consequences.get(\"pros\", [])\n",
    "                cons = adr.consequences.get(\"cons\", [])\n",
    "                if pros:\n",
    "                    consequences_text += \"**Pros:**\\n\" + \"\\n\".join(f\"- {item}\" for item in pros) + \"\\n\\n\"\n",
    "                if cons:\n",
    "                    consequences_text += \"**Cons:**\\n\" + \"\\n\".join(f\"- {item}\" for item in cons)\n",
    "            else:\n",
    "                consequences_text = adr.consequences\n",
    "\n",
    "            adr_text = f\"\"\"# ADR-{i:03d}: {adr.title}\n",
    "\n",
    "**Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "**Status**: Proposed\n",
    "\n",
    "## Context\n",
    "{adr.context}\n",
    "\n",
    "## Decision\n",
    "{adr.decision}\n",
    "\n",
    "## Consequences\n",
    "{consequences_text.strip()}\n",
    "\"\"\"\n",
    "            formatted_adrs_list.append(adr_text.strip())\n",
    "        \n",
    "        return formatted_adrs_list\n",
    "\n",
    "    def write_adrs(self, summary: str, feedback: str = None) -> (List[ADR], List[str]):\n",
    "        \"\"\"\n",
    "        Main method to generate and format ADRs.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - A list of the structured ADR Pydantic objects.\n",
    "            - A list of the formatted ADR strings.\n",
    "        \"\"\"\n",
    "        decisions_data = self._extract_design_decisions(summary, feedback)\n",
    "        if not decisions_data:\n",
    "            return [], [] # Return empty lists if extraction fails\n",
    "        \n",
    "        # print(type(decisions_data))\n",
    "        # print(decisions_data)\n",
    "        adrs = [ADR(**data) for data in decisions_data]\n",
    "        formatted_adrs = self._format_adrs(adrs)\n",
    "        return adrs, formatted_adrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ecb89",
   "metadata": {},
   "source": [
    "### ADR Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34be7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdrCheckerAgent:\n",
    "    \"\"\"\n",
    "    Verifies that a list of ADRs is logical, well-formed, and consistent\n",
    "    with the provided repository summary. Provides feedback for correction.\n",
    "    \"\"\"\n",
    "    def __init__(self,  model_name: str = \"gemini-2.5-pro\"):\n",
    "        print(\"Initializing AdrCheckerAgent...\")\n",
    "        self.model = LLMCaller(model_name)\n",
    "\n",
    "    def verify_adrs(self, summary: str, adrs: List[str]) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Verifies the ADRs against the repository summary.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - A boolean: True if the ADRs are deemed correct, False otherwise.\n",
    "            - A string: A justification if correct, or actionable feedback if incorrect.\n",
    "        \"\"\"\n",
    "        print(\"Verifying generated ADRs against the summary...\")\n",
    "        # Join the list of ADR strings into a single block for the prompt\n",
    "        adrs_text = \"\\n\\n---\\n\\n\".join(adrs)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a principal software architect reviewing a set of auto-generated\n",
    "        Architecture Decision Records (ADRs). Your task is to ensure the ADRs are logical,\n",
    "        well-written, and directly supported by the provided **Repository Summary**.\n",
    "\n",
    "        **Repository Summary:**\n",
    "        ---\n",
    "        {summary}\n",
    "        ---\n",
    "\n",
    "        **ADRs to Verify:**\n",
    "        ---\n",
    "        {adrs_text}\n",
    "        ---\n",
    "\n",
    "        Analyze the ADRs. Do they capture the most important architectural decisions\n",
    "        mentioned in the summary (e.g., choice of framework, database, architecture pattern)?\n",
    "        Are the 'Context', 'Decision', and 'Consequences' sections plausible for a project\n",
    "        as described in the summary?\n",
    "\n",
    "        Your final answer MUST begin with a single word: **CORRECT** or **INCORRECT**.\n",
    "        - If **CORRECT**, provide a brief justification.\n",
    "        - If **INCORRECT**, provide actionable **FEEDBACK** for the ADR writing agent.\n",
    "\n",
    "        Example CORRECT response:\n",
    "        CORRECT: The ADRs accurately capture the key decisions regarding the multi-module Maven structure and the use of AngularJS on the frontend.\n",
    "\n",
    "        Example INCORRECT response:\n",
    "        INCORRECT: ADR-2, which discusses PostgreSQL, is not supported by the summary that explicitly mentions an H2 embedded database.\n",
    "        FEEDBACK: Please correct ADR-2 to reflect the use of the H2 database as stated in the summary. Additionally, add a new ADR for the decision to use a multi-module Maven monorepo, as this is a key architectural choice mentioned in the summary.\n",
    "        \"\"\"\n",
    "        response = self.model.call(prompt)\n",
    "        result_text = response.strip()\n",
    "\n",
    "        is_correct = result_text.startswith(\"CORRECT\")\n",
    "        feedback = result_text\n",
    "\n",
    "        if is_correct:\n",
    "            print(\"-> Verification Result: ADRs are CORRECT\")\n",
    "        else:\n",
    "            print(\"-> Verification Result: ADRs are INCORRECT\")\n",
    "\n",
    "        return is_correct, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5a21b",
   "metadata": {},
   "source": [
    "### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent:\n",
    "    \"\"\"\n",
    "    Orchestrates the workflow with a self-correcting feedback loop for summarization.\n",
    "    1. Clone a repo.\n",
    "    2. Summarize it.\n",
    "    3. Verify the summary.\n",
    "    4. Generate ADRs from the verified summary.\n",
    "    \"\"\"\n",
    "    def __init__(self,  model_name: str = \"gemini-2.5-pro\"):\n",
    "        print(\"Initializing the Orchestrator Agent...\")\n",
    "        self.summarizer_agent = RepoSummarizer(model_name=model_name)\n",
    "        self.summary_checker_agent = SummaryCheckerAgent(model_name=model_name)\n",
    "        self.adr_writer_agent = AdrWriterAgent(model_name=model_name)\n",
    "        self.adr_checker_agent = AdrCheckerAgent(model_name=model_name)\n",
    "        print(\"Orchestrator is ready with all subordinate agents.\")\n",
    "\n",
    "    def _clone_repo(self, repo_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Clones a repository into a temporary directory.\n",
    "        Returns the path to the temporary directory.\n",
    "        \"\"\" \n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        try:\n",
    "            print(f\"Cloning {repo_url} into temporary directory {temp_dir}...\")\n",
    "            subprocess.check_call(['git', 'clone', repo_url, temp_dir], stderr=subprocess.DEVNULL)\n",
    "            print(\"Cloning successful.\")\n",
    "            return temp_dir\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(f\"Error cloning repository: {e}. Please check the URL and that Git is installed.\")\n",
    "            # Clean up the failed clone attempt\n",
    "            shutil.rmtree(temp_dir)\n",
    "            return None\n",
    "        \n",
    "    def _sanitize_filename(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper method now owned by the orchestrator.\n",
    "        \"\"\"\n",
    "        name = name.replace(' ', '_')\n",
    "        name = re.sub(r'[^\\w\\.-]', '', name)\n",
    "        return name[:100]\n",
    "\n",
    "    def _save_adrs(self, adrs: List[ADR], formatted_adrs: List[str], output_path: str):\n",
    "        \"\"\"\n",
    "        Saves the final, verified ADRs to disk.\n",
    "        \"\"\"\n",
    "        if not adrs:\n",
    "            print(\"No ADRs to save.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        print(f\"Saving final ADRs to directory: '{os.path.abspath(output_path)}'\")\n",
    "\n",
    "        for i, (adr_object, adr_content) in enumerate(zip(adrs, formatted_adrs), start=1):\n",
    "            safe_filename = self._sanitize_filename(adr_object.title)\n",
    "            filename = f\"{i:03d}_{safe_filename}.md\"\n",
    "            file_path = os.path.join(output_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(adr_content)\n",
    "                print(f\"  -> Successfully saved {filename}\")\n",
    "            except IOError as e:\n",
    "                print(f\"  -> Failed to save {filename}. Error: {e}\")\n",
    "\n",
    "    def run(self, repo_url: str, adr_output_path: str, max_attempts: int = 3):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ðŸš€ Starting Orchestration Workflow\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        repo_path = self._clone_repo(repo_url)\n",
    "        if not repo_path: return []\n",
    "\n",
    "        # --- LOOP 1: Summary Generation and Verification ---\n",
    "        summary = \"\"\n",
    "        summary_feedback = None\n",
    "        is_summary_correct = False\n",
    "        for attempt in range(max_attempts):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"ðŸ”¥ Summary Generation Attempt {attempt + 1} of {max_attempts}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            summary = self.summarizer_agent.summarize_repo(repo_path=repo_path, feedback=summary_feedback)\n",
    "            is_summary_correct, summary_feedback = self.summary_checker_agent.verify_summary(summary=summary, repo_path=repo_path)\n",
    "            print(f\"Feedback Received: {summary_feedback}\")\n",
    "\n",
    "            if is_summary_correct:\n",
    "                print(\"\\nâœ… Summary confirmed accurate. Proceeding to ADR generation.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\nâŒ Summary incorrect. Will attempt to regenerate.\")\n",
    "        \n",
    "        # if not is_summary_correct:\n",
    "        #     print(f\"\\nðŸš¨ Workflow Halted: Failed to generate an accurate summary after {max_attempts} attempts.\")\n",
    "        #     return []\n",
    "\n",
    "        # --- LOOP 2: ADR Generation and Verification ---\n",
    "        list_of_adr_objects = []\n",
    "        list_of_adr_strings = []\n",
    "        adr_feedback = None\n",
    "        are_adrs_correct = False\n",
    "        for attempt in range(max_attempts):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"ðŸ”¥ ADR Generation Attempt {attempt + 1} of {max_attempts}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # 1. GENERATE ADRs (content only, no saving yet)\n",
    "            list_of_adr_objects, list_of_adr_strings = self.adr_writer_agent.write_adrs(summary=summary, feedback=adr_feedback)\n",
    "            \n",
    "            # 2. VERIFY the generated ADRs\n",
    "            if not list_of_adr_strings:\n",
    "                print(\"ADR writer returned no content. Continuing attempt.\")\n",
    "                adr_feedback = \"The previous attempt returned no content. Please try again, ensuring you generate valid ADRs from the summary.\"\n",
    "                continue\n",
    "\n",
    "            are_adrs_correct, adr_feedback = self.adr_checker_agent.verify_adrs(summary=summary, adrs=list_of_adr_strings)\n",
    "            print(f\"Feedback Received: {adr_feedback}\")\n",
    "\n",
    "            if are_adrs_correct:\n",
    "                print(\"\\nâœ… ADRs confirmed accurate.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\nâŒ ADRs incorrect. Will attempt to regenerate.\")\n",
    "\n",
    "        # if not are_adrs_correct:\n",
    "        #     print(f\"\\nðŸš¨ Workflow Halted: Failed to generate accurate ADRs after {max_attempts} attempts.\")\n",
    "        #     return []\n",
    "\n",
    "        # --- FINAL STEP: Save the approved ADRs ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ðŸ’¾ Saving verified ADRs to disk...\")\n",
    "        self._save_adrs(list_of_adr_objects, list_of_adr_strings, adr_output_path)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ Orchestration Workflow Finished Successfully!\")\n",
    "        print(\"=\"*50)\n",
    "        return list_of_adr_strings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd8072",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc7d9d",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a0a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Orchestrator Agent...\n",
      "Initializing with model: gemini-2.5-pro\n",
      "Initializing LLMCaller with model: gemini-2.5-pro\n",
      "Initializing SummaryCheckerAgent...\n",
      "Initializing LLMCaller with model: gemini-2.5-pro\n",
      "Initializing AdrWriterAgent...\n",
      "Initializing LLMCaller with model: gemini-2.5-pro\n",
      "Initializing AdrCheckerAgent...\n",
      "Initializing LLMCaller with model: gemini-2.5-pro\n",
      "Orchestrator is ready with all subordinate agents.\n",
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Orchestration Workflow\n",
      "==================================================\n",
      "Cloning https://github.com/karthikv1392/cs6401_se.git into temporary directory C:\\Users\\rudra\\AppData\\Local\\Temp\\tmpw_u8apj4...\n",
      "Cloning successful.\n",
      "--------------------------------------------------\n",
      "ðŸ”¥ Summary Generation Attempt 1 of 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing repository structure...\n",
      "\n",
      "Summarizing the 5 largest files...\n",
      "  - Reading: Project\\project_1.md\n",
      "  - Reading: _site\\projects\\project-1.html\n",
      "  - Reading: Project\\project_2.md\n",
      "  - Reading: _layouts\\minimal.html\n",
      "  - Reading: course_policy.md\n",
      "Verifying summary against the code in C:\\Users\\rudra\\AppData\\Local\\Temp\\tmpw_u8apj4...\n",
      "-> Verification Result: CORRECT\n",
      "Feedback Received: CORRECT: The summary accurately identifies the repository as a Jekyll-based static site for a university course on Software Design and Architecture. It correctly deduces the technology stack, site structure, and the specific topics covered in the curriculum by referencing the file and directory names.\n",
      "\n",
      "âœ… Summary confirmed accurate. Proceeding to ADR generation.\n",
      "--------------------------------------------------\n",
      "ðŸ”¥ ADR Generation Attempt 1 of 2\n",
      "--------------------------------------------------\n",
      "Extracting design decisions from the summary...\n",
      "Successfully extracted 4 design decisions.\n",
      "Verifying generated ADRs against the summary...\n",
      "-> Verification Result: ADRs are CORRECT\n",
      "Feedback Received: CORRECT: The ADRs are a logical and accurate representation of the architectural decisions described in the repository summary. They correctly identify the four most significant choices: the adoption of Jekyll for static site generation, the 'Content-as-Code' strategy using Markdown, the deployment model via GitHub Pages, and the use of a modular templating system for the UI. Each ADR's context, decision, and consequences are directly supported by the summary's analysis.\n",
      "\n",
      "âœ… ADRs confirmed accurate.\n",
      "\n",
      "==================================================\n",
      "ðŸ’¾ Saving verified ADRs to disk...\n",
      "Saving final ADRs to directory: 'D:\\LAB\\ADR\\AgenticAdr\\Generated_ADRs\\karthikv1392_cs6401_se\\dir3'\n",
      "  -> Successfully saved 001_Adoption_of_a_Static_Site_Generator_Jekyll.md\n",
      "  -> Successfully saved 002_Content-as-Code_and_Version_Control_Integration.md\n",
      "  -> Successfully saved 003_Simplified_Deployment_and_Hosting_via_GitHub_Pages.md\n",
      "  -> Successfully saved 004_Component-Based_UI_via_a_Templating_System.md\n",
      "\n",
      "ðŸŽ‰ Orchestration Workflow Finished Successfully!\n",
      "==================================================\n",
      "\n",
      "Process complete. 4 ADRs were generated and saved.\n",
      "You can find the markdown files in the 'Generated_ADRs/karthikv1392_cs6401_se/dir3' folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your inputs\n",
    "repo_url_to_process = \"https://github.com/karthikv1392/cs6401_se.git\"\n",
    "output_directory_for_adrs = 'Generated_ADRs/' + repo_url_to_process[19:].removesuffix('.git').replace('/', '_')\n",
    "output_directory_for_adrs = output_directory_for_adrs + '/dir3'\n",
    "\n",
    "model_name = \"gemini-2.5-pro\"\n",
    "\n",
    "# Instantiate the Orchestrator Agent\n",
    "orchestrator = OrchestratorAgent(model_name=model_name)\n",
    "\n",
    "# Execute the entire workflow with a single call\n",
    "final_adrs = orchestrator.run(\n",
    "    repo_url=repo_url_to_process,\n",
    "    adr_output_path=output_directory_for_adrs,\n",
    "    max_attempts=3\n",
    ")\n",
    "\n",
    "# Final confirmation\n",
    "if final_adrs:\n",
    "    print(f\"\\nProcess complete. {len(final_adrs)} ADRs were generated and saved.\")\n",
    "    print(f\"You can find the markdown files in the '{output_directory_for_adrs}' folder.\")\n",
    "else:\n",
    "    print(\"\\nProcess finished, but no ADRs were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4bef8",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266581c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Orchestrator Agent...\n",
      "Initializing with model: gpt-5\n",
      "Initializing LLMCaller with model: gpt-5\n",
      "Initializing SummaryCheckerAgent...\n",
      "Initializing LLMCaller with model: gpt-5\n",
      "Initializing AdrWriterAgent...\n",
      "Initializing LLMCaller with model: gpt-5\n",
      "Initializing AdrCheckerAgent...\n",
      "Initializing LLMCaller with model: gpt-5\n",
      "Orchestrator is ready with all subordinate agents.\n",
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Orchestration Workflow\n",
      "==================================================\n",
      "Cloning https://github.com/karthikv1392/cs6401_se.git into temporary directory C:\\Users\\rudra\\AppData\\Local\\Temp\\tmpgodfe1wc...\n",
      "Cloning successful.\n",
      "--------------------------------------------------\n",
      "ðŸ”¥ Summary Generation Attempt 1 of 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing repository structure...\n",
      "\n",
      "Summarizing the 5 largest files...\n",
      "  - Reading: Project\\project_1.md\n",
      "  - Reading: _site\\projects\\project-1.html\n",
      "  - Reading: Project\\project_2.md\n",
      "  - Reading: _layouts\\minimal.html\n",
      "  - Reading: course_policy.md\n",
      "Verifying summary against the code in C:\\Users\\rudra\\AppData\\Local\\Temp\\tmpgodfe1wc...\n",
      "-> Verification Result: INCORRECT\n",
      "Feedback Received: INCORRECT: The summary includes several unverified specifics not supported by the provided file/directory listing.\n",
      "\n",
      "FEEDBACK: Revise the summary to stick to facts evident from the repository structure. \n",
      "- Keep: Itâ€™s a Jekyll-based course website (Jekyll config, collections _announcements/_modules/_schedules/_staffers, pages for lectures/tutorials/projects/policy, slides matching topics like refactoring, code smells/metrics, design patterns, architecture). Note that _site and a temporary Office file are committed.\n",
      "- Remove or qualify: â€œSpring 2025,â€ exact Gemfile dependencies/version (webrick ~> 1.7), detailed layout features (breadcrumbs/TOC/edit links), specific course policy rules (AI usage, plagiarism, late policy), and detailed project briefs or a â€œProject 1 mismatchâ€ claim. These are not verifiable from the structure alone.\n",
      "- If you mention dependencies or policies, frame them as â€œlikelyâ€ or omit them unless you verify the actual file contents.\n",
      "\n",
      "âŒ Summary incorrect. Will attempt to regenerate.\n",
      "--------------------------------------------------\n",
      "ðŸ”¥ Summary Generation Attempt 2 of 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Re-generating summary with feedback...\n",
      "\n",
      "Summarizing the 5 largest files...\n",
      "  - Reading: Project\\project_1.md\n",
      "  - Reading: _site\\projects\\project-1.html\n",
      "  - Reading: Project\\project_2.md\n",
      "  - Reading: _layouts\\minimal.html\n",
      "  - Reading: course_policy.md\n",
      "Verifying summary against the code in C:\\Users\\rudra\\AppData\\Local\\Temp\\tmpgodfe1wc...\n",
      "-> Verification Result: INCORRECT\n",
      "Feedback Received: INCORRECT: The summary makes unverified claims about specific gems and hosting.\n",
      "FEEDBACK: Do not assert specific dependencies or hosting without evidence. Replace â€œJekyll (via GitHub Pages)â€ and â€œRuby gems (from Gemfile): github-pages and webrickâ€ with neutral statements like â€œJekyll-based site with a Gemfile and Gemfile.lock.â€ If you want to list gems or hosting, read and cite the actual Gemfile contents or remove those specifics. The rest of the summary is accurate.\n",
      "\n",
      "âŒ Summary incorrect. Will attempt to regenerate.\n",
      "--------------------------------------------------\n",
      "ðŸ”¥ ADR Generation Attempt 1 of 2\n",
      "--------------------------------------------------\n",
      "Extracting design decisions from the summary...\n",
      "Successfully extracted 13 design decisions.\n",
      "Verifying generated ADRs against the summary...\n",
      "-> Verification Result: ADRs are CORRECT\n",
      "Feedback Received: CORRECT: The ADRs align with the repository summary for a Jekyll/GitHub Pages static course site. They cover the key decisions visible in the repo: generator/hosting choice, dependency pinning via github-pages and webrick, committing _site, use of Jekyll collections vs flat pages, custom layouts/SCSS, storing slide binaries, lack of strict .gitignore, asset inconsistencies from tracking _site, multiple landing pages, schedule as a collection, reliance on GitHub Pages builds, and staying within the plugin whitelist. Contexts are grounded in observed files, and the decision/consequence trade-offs are plausible for this setup.\n",
      "\n",
      "âœ… ADRs confirmed accurate.\n",
      "\n",
      "==================================================\n",
      "ðŸ’¾ Saving verified ADRs to disk...\n",
      "Saving final ADRs to directory: 'D:\\LAB\\ADR\\AgenticAdr\\Generated_ADRs\\karthikv1392_cs6401_se\\dir4'\n",
      "  -> Successfully saved 001_Static_site_generator_and_hosting.md\n",
      "  -> Successfully saved 002_Dependency_management_and_reproducibility.md\n",
      "  -> Successfully saved 003_Commit_generated_site_artifacts.md\n",
      "  -> Successfully saved 004_Content_modeling_with_Jekyll_collections.md\n",
      "  -> Successfully saved 005_Lectures_and_tutorials_modeled_as_flat_pages.md\n",
      "  -> Successfully saved 006_Custom_layouts_includes_and_SASS_theming.md\n",
      "  -> Successfully saved 007_Binary_slide_assets_stored_in_repository.md\n",
      "  -> Successfully saved 008_Lenient_ignore_policy_for_temporary_files.md\n",
      "  -> Successfully saved 009_Asset_source-of-truth_inconsistency.md\n",
      "  -> Successfully saved 010_Multiple_home-like_top-level_pages.md\n",
      "  -> Successfully saved 011_Schedule_modeled_as_a_collection_despite_small_size.md\n",
      "  -> Successfully saved 012_Rely_solely_on_GitHub_Pages_for_builds_no_external_CI.md\n",
      "  -> Successfully saved 013_Conformance_to_GitHub_Pages_plugin_whitelist.md\n",
      "\n",
      "ðŸŽ‰ Orchestration Workflow Finished Successfully!\n",
      "==================================================\n",
      "\n",
      "Process complete. 13 ADRs were generated and saved.\n",
      "You can find the markdown files in the 'Generated_ADRs/karthikv1392_cs6401_se/dir4' folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your inputs\n",
    "repo_url_to_process = \"https://github.com/karthikv1392/cs6401_se.git\"\n",
    "output_directory_for_adrs = 'Generated_ADRs/' + repo_url_to_process[19:].removesuffix('.git').replace('/', '_')\n",
    "output_directory_for_adrs = output_directory_for_adrs + '/dir4'\n",
    "\n",
    "model_name = \"gpt-5\"\n",
    "\n",
    "# Instantiate the Orchestrator Agent\n",
    "orchestrator = OrchestratorAgent(model_name=model_name)\n",
    "\n",
    "# Execute the entire workflow with a single call\n",
    "final_adrs = orchestrator.run(\n",
    "    repo_url=repo_url_to_process,\n",
    "    adr_output_path=output_directory_for_adrs,\n",
    "    max_attempts=3\n",
    ")\n",
    "\n",
    "# Final confirmation\n",
    "if final_adrs:\n",
    "    print(f\"\\nProcess complete. {len(final_adrs)} ADRs were generated and saved.\")\n",
    "    print(f\"You can find the markdown files in the '{output_directory_for_adrs}' folder.\")\n",
    "else:\n",
    "    print(\"\\nProcess finished, but no ADRs were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293b5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
